{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "from utils import load_param, save_param\n",
    "from load_data import nyu2_dataloaders\n",
    "from model.model import get_model\n",
    "from loss import compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define check_loss on_set\n",
    "purpose: check loss on validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_loss_on_set(dataloader, model, device):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in dataloader:\n",
    "            x_val = x_val.to(device=device)\n",
    "            y_val = y_val.to(device=device)\n",
    "            y_pred = model(x_val)\n",
    "            \n",
    "            _loss = compute_loss(pred=y_pred,\n",
    "                                 truth=y_val,\n",
    "                                 device=device,\n",
    "                                 _alpha=loss_params['_alpha'], \n",
    "                                 _lambda=loss_params['_lambda'], \n",
    "                                 _mu=loss_params['_mu'])\n",
    "            loss += _loss\n",
    "        loss /= len(dataloader)\n",
    "        print(\"Test on [val]: loss avg: %.4f\" \n",
    "              % (\n",
    "                    loss    \n",
    "                )\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'epochs': 25,\n",
    "    'lr': 1e-4,\n",
    "    'L2': 1e-4,\n",
    "    'batch_size': 32\n",
    "}\n",
    "loss_params = {\n",
    "    '_alpha': 0.5,\n",
    "    '_lambda': 1,\n",
    "    '_mu': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader,\n",
    "          val_dataloader,\n",
    "          model,\n",
    "          optimizer,\n",
    "          epochs,\n",
    "          device):\n",
    "    print_every = 5\n",
    "    \n",
    "    model = model.to(device=device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"train(): Training on {}\".format(device))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # batched_image_size: (batch_size, C, H, W)\n",
    "        for i, (x_tr, y_tr) in enumerate(tqdm(train_dataloader)):\n",
    "            # turn to train mode\n",
    "            model.train()\n",
    "            \n",
    "            x_tr = x_tr.to(device=device)\n",
    "            y_tr = y_tr.to(device=device)\n",
    "            y_pred = model(x_tr)\n",
    "            \n",
    "            loss = compute_loss(pred=y_pred,\n",
    "                                truth=y_tr,\n",
    "                                device=device,\n",
    "                                _alpha=loss_params['_alpha'], \n",
    "                                _lambda=loss_params['_lambda'], \n",
    "                                _mu=loss_params['_mu'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            end_time = time.time()\n",
    "            if i % print_every == 0:\n",
    "                # print the information of the epoch\n",
    "                print(\"[Epoch]: %d/%d [Iteration]: %d/%d, [loss]: %.4f, [Time Spent]: %.3f\"\n",
    "                      %(\n",
    "                            epoch, epochs, \n",
    "                            i, len(train_dataloader), \n",
    "                            loss, \n",
    "                            (end_time - start_time)\n",
    "                        )\n",
    "                      )\n",
    "                \n",
    "        # check on validation set each epoch\n",
    "        check_loss_on_set(dataloader=val_dataloader,\n",
    "                          model=model,\n",
    "                          device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define main session\n",
    "main() is actually the function does the setup and call train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # hyperparams    \n",
    "    # ---------------- params ---------------- #\n",
    "    epochs = hparams['epochs']\n",
    "    lr = hparams['lr']\n",
    "    weight_decay = hparams['L2']\n",
    "    # batchsize should better be more than 32 since BN is used frequently\n",
    "    batchsize = hparams['batch_size']\n",
    "    # ---------------- params ---------------- #\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    print(\"main(): Getting model......\")\n",
    "    model = get_model(encoder='resnet50')\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    \n",
    "    print(\"main(): Getting dataloaders......\")\n",
    "    train_set, val_set, test_set = nyu2_dataloaders(batchsize=batchsize,\n",
    "                                             nyu2_path='./nyu2_train')\n",
    "    \n",
    "    print(\"main(): start training......\")\n",
    "    # all epochs wrapped in train()\n",
    "    train(train_dataloader=train_set,\n",
    "          val_dataloader=val_set,\n",
    "          model=model,\n",
    "          optimizer=optimizer,\n",
    "          epochs=epochs,\n",
    "          device=device)\n",
    "    \n",
    "    print(\"Training Session is over, test the model on testset\")\n",
    "    # after training, test it on testset\n",
    "    check_loss_on_set(dataloader=test_set,\n",
    "                      model=model,\n",
    "                      device=device)\n",
    "    \n",
    "    # SAVE THE PARAMETERS\n",
    "    # default is current time, change it whatever you like\n",
    "    filelabel = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "    save_param(model=model,\n",
    "               pth_path='./model_pth/{}.pth'.format(filelabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main(): Getting model......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to pretrained_model/resnet50/resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:27<00:00, 3.69MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main(): Getting dataloaders......\n",
      "Entering nyu2_dataloaders()\n",
      "---------------- Loading Dataloaders ----------------\n",
      "-------- Datasets are ready, preparing Dataloaders --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/transforms.py:317: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v6/q6kwcrrn67j9yz1yx4ckcd900000gn/T/ipykernel_12597/451043146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/v6/q6kwcrrn67j9yz1yx4ckcd900000gn/T/ipykernel_12597/2303526470.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"main(): Getting dataloaders......\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     train_set, val_set, test_set = nyu2_dataloaders(batchsize=batchsize,\n\u001b[0m\u001b[1;32m     23\u001b[0m                                              nyu2_path='./nyu2_train')\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/gitclones/DIP-proj-DepthEstimation/load_data.py\u001b[0m in \u001b[0;36mnyu2_dataloaders\u001b[0;34m(batchsize, nyu2_path)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# datalodaers, to be enumerated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     train_loader  = DataLoader (dataset=train_dataset,\n\u001b[0m\u001b[1;32m    107\u001b[0m                                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                                 batch_size=batchsize)\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    103\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
